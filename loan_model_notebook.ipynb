{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf5d4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Tuple\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc928f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFileAndLoader(object):\n",
    "    def __init__(self):\n",
    "        self.data_file_path = None\n",
    "\n",
    "    def dataset_file_path(self):\n",
    "        try:\n",
    "            if self.data_file_path is None:\n",
    "                \n",
    "                DATASET_DIR_PATH = 'copy/the/relative/path/to/the/dataset/folder/here/'\n",
    "                \n",
    "                self.data_file_path = os.path.join(DATASET_DIR_PATH, 'train.csv')\n",
    "                print(f\"Data file path: {self.data_file_path}\")\n",
    "            return self.data_file_path\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "    def load_dataset_file(self):\n",
    "        load_dataset_file = pd.read_csv(self.dataset_file_path())\n",
    "        print(f\"Loaded dataset: {load_dataset_file}\")\n",
    "        return load_dataset_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2526c449",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessing(object):\n",
    "    def __init__(self, load_dataset_file):\n",
    "        self.load_dataset = load_dataset_file\n",
    "        self.gender_loan_status = None\n",
    "\n",
    "        # Remove the 'Loan_ID' column\n",
    "        self.load_dataset.drop('Loan_ID', axis=1, inplace=True)\n",
    "\n",
    "    def data_filling_empty(self):\n",
    "\n",
    "        # Filling up the missing values in the columns and rows affected\n",
    "        self.load_dataset['Gender'] = self.load_dataset['Gender'].fillna(self.load_dataset['Gender'].mode()[0])\n",
    "        self.load_dataset['Married'] = self.load_dataset['Married'].fillna(self.load_dataset['Married'].mode()[0])\n",
    "        self.load_dataset['Dependents'] = self.load_dataset['Dependents'].str.replace(r\"\\+\", \"\", regex=True)\n",
    "        self.load_dataset['Dependents'] = pd.to_numeric(self.load_dataset['Dependents'])\n",
    "        self.load_dataset['Dependents'] = self.load_dataset['Dependents'].fillna(self.load_dataset['Dependents'].mode()[0])\n",
    "        self.load_dataset['Self_Employed'] = self.load_dataset['Self_Employed'].fillna(self.load_dataset['Self_Employed'].mode()[0])\n",
    "        self.load_dataset['LoanAmount'] = self.load_dataset['LoanAmount'].fillna(self.load_dataset['LoanAmount'].mean())\n",
    "        self.load_dataset['Loan_Amount_Term'] = self.load_dataset['Loan_Amount_Term'].fillna(self.load_dataset['Loan_Amount_Term'].mean())\n",
    "        self.load_dataset['Credit_History'] = self.load_dataset['Credit_History'].fillna(self.load_dataset['Credit_History'].mean())\n",
    "\n",
    "        self.gender_loan_status = self.load_dataset[['Gender', 'Loan_Status']]\n",
    "\n",
    "        return self.load_dataset\n",
    "\n",
    "    def data_encoding(self):\n",
    "        # label encoding\n",
    "        self.load_dataset['Gender'] = self.load_dataset['Gender'].map({'Male': 0, 'Female': 1}).astype(int)\n",
    "        self.load_dataset['Married'] = self.load_dataset['Married'].map({'No': 0, 'Yes': 1}).astype(int)\n",
    "        self.load_dataset['Dependents'] = self.load_dataset['Dependents'].astype(int)\n",
    "        self.load_dataset['Education'] = self.load_dataset['Education'].map({'Not Graduate': 0, 'Graduate': 1}).astype(int)\n",
    "        self.load_dataset['Self_Employed'] = self.load_dataset['Self_Employed'].map({'No': 0, 'Yes': 1}).astype(int)\n",
    "        self.load_dataset['ApplicantIncome'] = self.load_dataset['ApplicantIncome'].astype(int)\n",
    "        self.load_dataset['CoApplicantIncome'] = self.load_dataset['CoApplicantIncome'].astype(int)\n",
    "        self.load_dataset['Credit_History'] = self.load_dataset['Credit_History'].astype(int)\n",
    "        self.load_dataset['Property_Area'] = self.load_dataset['Property_Area'].map({'Urban': 0, 'Rural': 1, 'SemiUrban': 2}).astype(int)\n",
    "        self.load_dataset['Loan_Status'] = self.load_dataset['Loan_Status'].map({'N': 0, 'Y': 1}).astype(int)\n",
    "        return self.load_dataset\n",
    "\n",
    "    def scaling_the_data(self):\n",
    "        # scale the data\n",
    "        scaler = StandardScaler()\n",
    "        # split into features and target\n",
    "        X = self.load_dataset.drop('Loan_Status', axis=1)\n",
    "        X = scaler.fit_transform(X)\n",
    "        Y = self.load_dataset['Loan_Status']\n",
    "        # handling class imbalance using SMOTE\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_resampled, y_resampled = smote.fit_resample(X, Y)\n",
    "        return X_resampled, y_resampled, scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7166d4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer(object):\n",
    "    \"\"\"\n",
    "    This class is used to train the model and evaluate the model performance.\n",
    "    It also provides the hyperparameter tuning for individual models\n",
    "    It is initialized with the resampled data\n",
    "    \"\"\"\n",
    "    def __init__(self, x_resampled, y_resampled):\n",
    "        self.x_resampled = x_resampled\n",
    "        self.y_resampled = y_resampled\n",
    "\n",
    "    def train_split(self, test_size=0.2, random_state=4):\n",
    "        \"\"\"\n",
    "        This function is used to split the data into train and test\n",
    "        :param test_size:\n",
    "        :param random_state:\n",
    "        :return train_x, train_y, test_x, test_y:\n",
    "        \"\"\"\n",
    "        # # split into train and test\n",
    "        x_train, x_test, y_train, y_test = train_test_split(self.x_resampled, self.y_resampled, test_size=test_size, random_state=random_state)\n",
    "\n",
    "        # model training and evaluation for individual models\n",
    "        rf_params = {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [10, 20, 30],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4]\n",
    "        }\n",
    "        ada_params = {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'learning_rate': [0.01, 0.1, 1.0]\n",
    "        }\n",
    "        grd_params = {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'learning_rate': [0.01, 0.1, 1.0],\n",
    "            'max_depth': [3, 5, 7]\n",
    "        }\n",
    "        log_params = {\n",
    "            'C': [0.1, 1, 10]\n",
    "        }\n",
    "        svm_params = {\n",
    "            'C': [0.1, 1, 10],\n",
    "            'kernel': ['linear', 'rbf'],\n",
    "            'gamma': ['scale', 'auto']\n",
    "        }\n",
    "        return rf_params, ada_params, grd_params, log_params, svm_params, x_train, x_test, y_train, y_test\n",
    "\n",
    "    def classifiers_train_models(self, rf_params, ada_params, grd_params, log_params, svm_params, x_train, x_test, y_train, y_test):\n",
    "        # Create a dictionary of classifiers and their respective parameter grids\n",
    "        rand_classifiers = {\n",
    "            'RandomForestClassifier': (RandomForestClassifier(), rf_params),\n",
    "            'AdaBoostClassifier': (AdaBoostClassifier(), ada_params),\n",
    "            'GradientBoostingClassifier': (GradientBoostingClassifier(), grd_params),\n",
    "            'LogisticRegression': (LogisticRegression(), log_params),\n",
    "            'SVC': (SVC(), svm_params)\n",
    "        }\n",
    "        # Train and evaluate each classifier\n",
    "        rand_best_classifiers = {}\n",
    "        # Loop through each classifier and perform RandomizedSearchCV\n",
    "        for name, (classifier, params) in rand_classifiers.items():\n",
    "            random_search = RandomizedSearchCV(classifier, param_distributions=params, n_iter=10, cv=5, n_jobs=-1, random_state=42)\n",
    "            random_search.fit(x_train, y_train)\n",
    "            rand_best_classifiers[name] = random_search.best_estimator_\n",
    "            print(f\"{name} best parameters: {random_search.best_params_}\")\n",
    "            print(f\"{name} best score: {random_search.best_score_}\")\n",
    "        return rand_best_classifiers, x_test, y_test,  x_train, y_train\n",
    "\n",
    "    def evaluate_models(self, rand_best_classifiers, x_test, y_test, x_train, y_train):\n",
    "        # Evaluate the performance of each classifier\n",
    "        for name, classifier in rand_best_classifiers.items():\n",
    "            classifier.fit(x_train, y_train).predict(x_test)\n",
    "            y_pred = classifier.predict(x_test)\n",
    "            print(f\"{name} accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "            print(f\"{name} precision: {precision_score(y_test, y_pred)}\")\n",
    "            print(f\"{name} f1 score: {f1_score(y_test, y_pred)}\")\n",
    "\n",
    "    def get_classifier(self, rand_best_classifiers, x_test, y_test, x_train, y_train):\n",
    "        random_forest_classifier = rand_best_classifiers['RandomForestClassifier']\n",
    "        adaboost_classifier = rand_best_classifiers['AdaBoostClassifier']\n",
    "        gradient_boosting_classifier = rand_best_classifiers['GradientBoostingClassifier']\n",
    "        logistic_regression = rand_best_classifiers['LogisticRegression']\n",
    "        naive_bayes_classifier = GaussianNB()\n",
    "        svm_classifier = rand_best_classifiers['SVC']\n",
    "\n",
    "        # Cross-validation\n",
    "        classifiers = {'Random Forest': random_forest_classifier, 'Adaboost': adaboost_classifier,\n",
    "                       'Gradient Boosting': gradient_boosting_classifier, 'Logistic Regression': logistic_regression,\n",
    "                       'Naive Bayes': naive_bayes_classifier, 'SVM': svm_classifier}\n",
    "        accuracy_scores = {}\n",
    "        improved_classifiers = {}\n",
    "        best_accuracy = 0\n",
    "        for name, classifier in classifiers.items():\n",
    "            scores = cross_val_score(classifier, x_train, y_train, cv=10, scoring='accuracy')\n",
    "            mean_score = scores.mean()\n",
    "            accuracy_scores[name] = mean_score\n",
    "            if mean_score > best_accuracy:\n",
    "                best_accuracy = mean_score\n",
    "                improved_classifiers = {name: classifier}\n",
    "            elif mean_score == best_accuracy:\n",
    "                improved_classifiers[name] = classifier\n",
    "        # Print the accuracies and improved classifiers\n",
    "        print(\"Classifier Accuracies:\", accuracy_scores)\n",
    "        for name, accuracy in accuracy_scores.items():\n",
    "            print(f\"{name} accuracy: {accuracy}\")\n",
    "        return improved_classifiers, x_test, y_test,  x_train, y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcef202c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleModel(object):\n",
    "    \"\"\"\n",
    "    Initialize the EnsembleModel class with the improved classifiers, test data, and training data\n",
    "    Parameters:\n",
    "        improved_classifiers (dict): A dictionary of the improved classifiers\n",
    "        x_test (numpy.ndarray): The test data\n",
    "        y_test (numpy.ndarray): The test labels\n",
    "        x_train (numpy.ndarray): The training data\n",
    "        y_train (numpy.ndarray): The training labels\n",
    "    \"\"\"\n",
    "    def __init__(self, improved_classifiers, x_test, y_test, x_train, y_train):\n",
    "        self.improved_classifiers = improved_classifiers\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "    def get_ensemble_model(self):\n",
    "        \"\"\"\n",
    "        Get the ensemble model from the improved classifiers\n",
    "        :return:\n",
    "        ensemble_accuracy:\n",
    "        f1_ensemble_accuracy:\n",
    "        precision_score_accuracy:\n",
    "        meta_predictions:\n",
    "        \"\"\"\n",
    "\n",
    "        meta_ensemble = VotingClassifier(estimators=[\n",
    "            (name, classifier) for name, classifier in self.improved_classifiers.items()], voting='hard')\n",
    "        meta_ensemble.fit(self.x_train, self.y_train)\n",
    "        meta_predictions = meta_ensemble.predict(self.x_test)\n",
    "\n",
    "        result_df = pd.DataFrame({'Actual': self.y_test, 'Predicted': meta_predictions})\n",
    "\n",
    "        ensemble_accuracy = accuracy_score(self.y_test, meta_predictions)\n",
    "        f1_ensemble_accuracy = f1_score(self.y_test, meta_predictions)\n",
    "        precision_score_accuracy = precision_score(self.y_test, meta_predictions)\n",
    "\n",
    "        return ensemble_accuracy, f1_ensemble_accuracy, precision_score_accuracy, meta_ensemble, result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74fcf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelStore(object):\n",
    "    \"\"\"\n",
    "    Initialize ModelStore with meta_ensemble_model and scaler_model.\n",
    "\n",
    "    Args:\n",
    "        meta_ensemble_model: The meta ensemble model.\n",
    "        scaler_model: The scaler model.\n",
    "    \"\"\"\n",
    "    def __init__(self, meta_ensemble_model, scaler_model):\n",
    "        self.meta_predictions = meta_ensemble_model\n",
    "        self.scaler = scaler_model\n",
    "\n",
    "    def store_model(self):\n",
    "        \"\"\"\n",
    "        Prompt the user for model and scaler filenames and store the models to files.\n",
    "        \"\"\"\n",
    "        model_filename, scaler_filename = self.name_file()\n",
    "        \n",
    "        PICKLES_DIR_PATH = 'copy/the/relative/path/to/the/pickles/folder/here/'\n",
    "\n",
    "        scaler_file_dir = os.path.join(PICKLES_DIR_PATH, scaler_filename)\n",
    "        model_file_dir = os.path.join(PICKLES_DIR_PATH, model_filename)\n",
    "\n",
    "        joblib.dump(self.scaler, scaler_file_dir)\n",
    "        print(f\"Scaler was successfully saved to {scaler_file_dir}\")\n",
    "        joblib.dump(self.meta_predictions, model_file_dir)\n",
    "        print(f\"Model was successfully saved to {model_file_dir}\")\n",
    "\n",
    "    def name_file(self) -> Tuple[str, str]:\n",
    "        \"\"\"\n",
    "        Prompt the user for model and scaler filenames.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[str, str]: A tuple containing the model and scaler filenames.\n",
    "        \"\"\"\n",
    "        default_model_name = \"model.pkl\"\n",
    "        default_scaler_name = \"scaler.pkl\"\n",
    "\n",
    "        model_name = default_model_name\n",
    "        scaler_name = default_scaler_name\n",
    "        return model_name, scaler_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244c33df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "data_loader = DataFileAndLoader()\n",
    "load_dataset = data_loader.load_dataset_file()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3078548f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processor = DataProcessing(load_dataset)\n",
    "data_processor.data_filling_empty()\n",
    "data_processor.data_encoding()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154092ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sampled, y_sampled, scaler = data_processor.scaling_the_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507aa23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer = ModelTrainer(x_sampled, y_sampled)\n",
    "rf_params, ada_params, grd_params, log_params, svm_params, x_train, x_test, y_train, y_test = model_trainer.train_split()\n",
    "model_classifiers, x_test, y_test, x_train, y_train = model_trainer.classifiers_train_models(rf_params, ada_params, grd_params, log_params, svm_params, x_train, x_test, y_train, y_test)\n",
    "improved_classifier, x_test, y_test, x_train, y_train = model_trainer.get_classifier(model_classifiers, x_test, y_test, x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25e4c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model = EnsembleModel(improved_classifier, x_test, y_test, x_train, y_train)\n",
    "ensemble_model_accuracy, f1_ensemble_model_accuracy, precision_score_model_accuracy, meta_model_ensemble, result_dataframe = ensemble_model.get_ensemble_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8082473c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dataframe_merge = pd.merge(result_dataframe, data_processor.gender_loan_status, left_index=True, right_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d07afb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar chart to visualize the rate of loan status repayment for females and males\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='Gender', hue='Predicted', data=result_dataframe_merge)\n",
    "plt.title('Loan Status Repayment by Gender')\n",
    "plt.xlabel('Gender')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322a10fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Ensemble accuracy: {ensemble_model_accuracy}\")\n",
    "print(f\"Ensemble f1 score: {f1_ensemble_model_accuracy}\")\n",
    "print(f\"Ensemble precision score: {precision_score_model_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7761778",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_store = ModelStore(meta_model_ensemble, scaler)\n",
    "model_store.store_model()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
